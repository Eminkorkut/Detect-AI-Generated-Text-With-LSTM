{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0490d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/emin/.local/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/emin/.local/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: tqdm in /home/emin/.local/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: joblib in /home/emin/.local/lib/python3.10/site-packages (from nltk) (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f46c079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/emin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/emin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/emin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/emin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')      \n",
    "nltk.download('wordnet')    \n",
    "nltk.download('omw-1.4') \n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "827dcde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80122f2c",
   "metadata": {},
   "source": [
    "1-Ai 0-Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4896af59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Car-free cities have become a subject of incre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Car Free Cities  Car-free cities, a concept ga...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Sustainable Urban Future  Car-free cities ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pioneering Sustainable Urban Living  In an e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Path to Sustainable Urban Living  In an ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29140</th>\n",
       "      <td>There has been a fuss about the Elector Colleg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29141</th>\n",
       "      <td>Limiting car usage has many advantages. Such a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29142</th>\n",
       "      <td>There's a new trend that has been developing f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29143</th>\n",
       "      <td>As we all know cars are a big part of our soci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29144</th>\n",
       "      <td>Cars have been around since the 1800's and hav...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29145 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  generated\n",
       "0      Car-free cities have become a subject of incre...          1\n",
       "1      Car Free Cities  Car-free cities, a concept ga...          1\n",
       "2        A Sustainable Urban Future  Car-free cities ...          1\n",
       "3        Pioneering Sustainable Urban Living  In an e...          1\n",
       "4        The Path to Sustainable Urban Living  In an ...          1\n",
       "...                                                  ...        ...\n",
       "29140  There has been a fuss about the Elector Colleg...          0\n",
       "29141  Limiting car usage has many advantages. Such a...          0\n",
       "29142  There's a new trend that has been developing f...          0\n",
       "29143  As we all know cars are a big part of our soci...          0\n",
       "29144  Cars have been around since the 1800's and hav...          0\n",
       "\n",
       "[29145 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Training_Essay_Data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a7022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43053c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_basic_stop_word = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0fcbfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff01bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    filtered_words = [word for word in lemmatized_words if word not in english_basic_stop_word]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04bf7d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>carfree city subject increasing debate year ur...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>car free city carfree city concept gaining tra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sustainable urban future carfree city emerging...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pioneering sustainable urban living era marked...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>path sustainable urban living age rapid urbani...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29140</th>\n",
       "      <td>ha fuss elector college people confused work s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29141</th>\n",
       "      <td>limiting car usage ha advantage putting lot po...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29142</th>\n",
       "      <td>trend ha developing year throttle affect mass ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29143</th>\n",
       "      <td>car big society today car bigger impact people...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29144</th>\n",
       "      <td>car 1800s popular year number car bought licen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29145 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  generated\n",
       "0      carfree city subject increasing debate year ur...          1\n",
       "1      car free city carfree city concept gaining tra...          1\n",
       "2      sustainable urban future carfree city emerging...          1\n",
       "3      pioneering sustainable urban living era marked...          1\n",
       "4      path sustainable urban living age rapid urbani...          1\n",
       "...                                                  ...        ...\n",
       "29140  ha fuss elector college people confused work s...          0\n",
       "29141  limiting car usage ha advantage putting lot po...          0\n",
       "29142  trend ha developing year throttle affect mass ...          0\n",
       "29143  car big society today car bigger impact people...          0\n",
       "29144  car 1800s popular year number car bought licen...          0\n",
       "\n",
       "[29145 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(preprocess_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a988ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carfree city subject increasing debate year urban area grapple challenge congestion pollution limited resource concept carfree city involves creating urban environment private automobile restricted completely banned focus alternative transportation method sustainable urban planning essay explores benefit challenge potential solution idea carfree city benefit carfree city environmental sustainability carfree city promote environmental sustainability reducing air pollution greenhouse gas emission fewer car road cleaner air decrease contribution global warming improved public health reduction automobile usage lead public health outcome fewer car road result fewer accident safer urban environment pedestrian cyclist air pollution lead reduced respiratory cardiovascular problem efficient space carfree city utilize urban space efficiently parking lot wide road repurposed green space park pedestrian zone enhancing quality life city reduced traffic congestion eliminating restricting car usage traffic congestion reduced leading faster commute time frustration resident commuter cost saving car ownership maintenance expensive carfree city resident save money vehicle fuel insurance improving financial wellbeing challenge carfree city resistance change transitioning carfree city face resistance citizen rely heavily car daily activity commute public transportation infrastructure effective public transportation crucial success carfree city city invest expand public transportation network ensure people viable alternative car economic impact business rely cardependent customer experience decline revenue carfree city essential address economic impact support business transition urban planning infrastructure redesign urban area carfree living requires planning investment infrastructure lengthy complex process solution carfree city expand public transportation invest expansion improvement public transportation provide convenient affordable alternative private car promote active transportation encourage walking cycling building bike lane pedestrianfriendly street ensuring safe infrastructure activity implement carpooling ridesharing promote carpooling ridesharing service reduce number private vehicle road adopt electric sustainable transportation encourage electric vehicle promote sustainability renewable energy source power transportation education public awareness educate citizen benefit carfree city involve planning process increase support understanding conclusion carfree city represent vision sustainable healthier urban future transition carfree city challenge potential benefit term environmental sustainability public health efficient urban living compelling goal combination investment public transportation promotion alternative transportation method public engagement city work future car longer dominant mode transportation carfree city offer promising vision sustainable vibrant urban environment'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1216e20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emin/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce18c88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b16f7206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7718,\n",
       " 5787,\n",
       " 1748,\n",
       " 2426,\n",
       " 3649,\n",
       " 4384,\n",
       " 614,\n",
       " 7876,\n",
       " 1989,\n",
       " 42146,\n",
       " 4427,\n",
       " 28014,\n",
       " 12231,\n",
       " 3614,\n",
       " 8271,\n",
       " 3721,\n",
       " 1097,\n",
       " 5787,\n",
       " 1748,\n",
       " 9018,\n",
       " 4441,\n",
       " 7876,\n",
       " 2858,\n",
       " 2839,\n",
       " 27930,\n",
       " 10770,\n",
       " 3190,\n",
       " 9301,\n",
       " 2962,\n",
       " 5559,\n",
       " 9358,\n",
       " 2446,\n",
       " 13347,\n",
       " 7876,\n",
       " 5410,\n",
       " 14268,\n",
       " 25409,\n",
       " 4414,\n",
       " 4427,\n",
       " 2785,\n",
       " 4610,\n",
       " 2126,\n",
       " 1097,\n",
       " 5787,\n",
       " 1748,\n",
       " 4414,\n",
       " 1097,\n",
       " 5787,\n",
       " 1748,\n",
       " 6142,\n",
       " 26809,\n",
       " 1097,\n",
       " 5787,\n",
       " 1748,\n",
       " 7719,\n",
       " 6142,\n",
       " 26809,\n",
       " 8868,\n",
       " 1633,\n",
       " 12231,\n",
       " 16325,\n",
       " 3623,\n",
       " 25592,\n",
       " 7380,\n",
       " 1097,\n",
       " 2975,\n",
       " 21723,\n",
       " 1633,\n",
       " 10070,\n",
       " 10156,\n",
       " 3298,\n",
       " 9917,\n",
       " 6596,\n",
       " 1171,\n",
       " 1535,\n",
       " 7741,\n",
       " 27930,\n",
       " 8748,\n",
       " 1085,\n",
       " 1171,\n",
       " 1535,\n",
       " 8055,\n",
       " 7380,\n",
       " 1097,\n",
       " 2975,\n",
       " 1255,\n",
       " 7380,\n",
       " 5778,\n",
       " 14178,\n",
       " 7876,\n",
       " 2858,\n",
       " 22382,\n",
       " 34632,\n",
       " 1633,\n",
       " 12231,\n",
       " 1085,\n",
       " 5322,\n",
       " 22949,\n",
       " 21134,\n",
       " 1917,\n",
       " 6942,\n",
       " 2272,\n",
       " 1097,\n",
       " 5787,\n",
       " 1748,\n",
       " 17624,\n",
       " 7876,\n",
       " 2272,\n",
       " 18306,\n",
       " 7647,\n",
       " 1256,\n",
       " 3094,\n",
       " 2975,\n",
       " 1128,\n",
       " 333,\n",
       " 29813,\n",
       " 4077,\n",
       " 2272,\n",
       " 3952,\n",
       " 22382,\n",
       " 6516,\n",
       " 27496,\n",
       " 3081,\n",
       " 1204,\n",
       " 1748,\n",
       " 5322,\n",
       " 4979,\n",
       " 28014,\n",
       " 18591,\n",
       " 28267,\n",
       " 1097,\n",
       " 8748,\n",
       " 4979,\n",
       " 28014,\n",
       " 5322,\n",
       " 3756,\n",
       " 5443,\n",
       " 31099,\n",
       " 640,\n",
       " 14285,\n",
       " 6623,\n",
       " 40718,\n",
       " 1575,\n",
       " 8914,\n",
       " 1097,\n",
       " 9238,\n",
       " 9262,\n",
       " 5789,\n",
       " 1097,\n",
       " 5787,\n",
       " 1748,\n",
       " 6623,\n",
       " 3613,\n",
       " 1637,\n",
       " 4038,\n",
       " 5252,\n",
       " 5096,\n",
       " 10068,\n",
       " 3176,\n",
       " 40013,\n",
       " 4427,\n",
       " 1097,\n",
       " 5787,\n",
       " 1748,\n",
       " 6625,\n",
       " 1487,\n",
       " 37005,\n",
       " 1097,\n",
       " 5787,\n",
       " 1748,\n",
       " 1986,\n",
       " 6625,\n",
       " 9511,\n",
       " 8814,\n",
       " 7272,\n",
       " 1097,\n",
       " 4445,\n",
       " 3842,\n",
       " 31099,\n",
       " 1171,\n",
       " 9358,\n",
       " 6884,\n",
       " 4050,\n",
       " 1171,\n",
       " 9358,\n",
       " 8780,\n",
       " 1943,\n",
       " 1097,\n",
       " 5787,\n",
       " 1748,\n",
       " 1748,\n",
       " 1325,\n",
       " 4292,\n",
       " 1171,\n",
       " 9358,\n",
       " 3127,\n",
       " 4155,\n",
       " 661,\n",
       " 13971,\n",
       " 5559,\n",
       " 1097,\n",
       " 3034,\n",
       " 2928,\n",
       " 1597,\n",
       " 8814,\n",
       " 2657,\n",
       " 8682,\n",
       " 6491,\n",
       " 1998,\n",
       " 7794,\n",
       " 6426,\n",
       " 1097,\n",
       " 5787,\n",
       " 1748,\n",
       " 6393,\n",
       " 2209,\n",
       " 3034,\n",
       " 2928,\n",
       " 1104,\n",
       " 1597,\n",
       " 6801,\n",
       " 7876,\n",
       " 5410,\n",
       " 6884,\n",
       " 27135,\n",
       " 7876,\n",
       " 1989,\n",
       " 1097,\n",
       " 5787,\n",
       " 2877,\n",
       " 4433,\n",
       " 5410,\n",
       " 4896,\n",
       " 6884,\n",
       " 16452,\n",
       " 3716,\n",
       " 1429,\n",
       " 4610,\n",
       " 1097,\n",
       " 5787,\n",
       " 1748,\n",
       " 4292,\n",
       " 1171,\n",
       " 9358,\n",
       " 1325,\n",
       " 7118,\n",
       " 9025,\n",
       " 1171,\n",
       " 9358,\n",
       " 2148,\n",
       " 11282,\n",
       " 10935,\n",
       " 5559,\n",
       " 2839,\n",
       " 1097,\n",
       " 7719,\n",
       " 4075,\n",
       " 9358,\n",
       " 7898,\n",
       " 6155,\n",
       " 16259,\n",
       " 2615,\n",
       " 7161,\n",
       " 11193,\n",
       " 22382,\n",
       " 13120,\n",
       " 4675,\n",
       " 13359,\n",
       " 3338,\n",
       " 6884,\n",
       " 3842,\n",
       " 3494,\n",
       " 1097,\n",
       " 7742,\n",
       " 278,\n",
       " 17445,\n",
       " 71,\n",
       " 1723,\n",
       " 7719,\n",
       " 1097,\n",
       " 7742,\n",
       " 278,\n",
       " 17445,\n",
       " 71,\n",
       " 1723,\n",
       " 2139,\n",
       " 4646,\n",
       " 1271,\n",
       " 2839,\n",
       " 4038,\n",
       " 2975,\n",
       " 11206,\n",
       " 5186,\n",
       " 13347,\n",
       " 9358,\n",
       " 7898,\n",
       " 5186,\n",
       " 4038,\n",
       " 7719,\n",
       " 26809,\n",
       " 15713,\n",
       " 2568,\n",
       " 2723,\n",
       " 1176,\n",
       " 9358,\n",
       " 3707,\n",
       " 1171,\n",
       " 9359,\n",
       " 20771,\n",
       " 9511,\n",
       " 4414,\n",
       " 1097,\n",
       " 5787,\n",
       " 1748,\n",
       " 6211,\n",
       " 5410,\n",
       " 1429,\n",
       " 2620,\n",
       " 1104,\n",
       " 4547,\n",
       " 7664,\n",
       " 1097,\n",
       " 5787,\n",
       " 1748,\n",
       " 2380,\n",
       " 5761,\n",
       " 13347,\n",
       " 22841,\n",
       " 7876,\n",
       " 2003,\n",
       " 6801,\n",
       " 1097,\n",
       " 5787,\n",
       " 1748,\n",
       " 4427,\n",
       " 2785,\n",
       " 4414,\n",
       " 3381,\n",
       " 6142,\n",
       " 26809,\n",
       " 1171,\n",
       " 1535,\n",
       " 6942,\n",
       " 7876,\n",
       " 2877,\n",
       " 13206,\n",
       " 3061,\n",
       " 6087,\n",
       " 4896,\n",
       " 1171,\n",
       " 9358,\n",
       " 12148,\n",
       " 5559,\n",
       " 9358,\n",
       " 2446,\n",
       " 1171,\n",
       " 12352,\n",
       " 1748,\n",
       " 670,\n",
       " 2003,\n",
       " 1097,\n",
       " 2392,\n",
       " 11410,\n",
       " 4235,\n",
       " 9358,\n",
       " 1097,\n",
       " 5787,\n",
       " 1748,\n",
       " 2897,\n",
       " 11781,\n",
       " 5761,\n",
       " 13347,\n",
       " 21266,\n",
       " 7876,\n",
       " 2858]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(df[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0900b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1633 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "      <th>encoded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>carfree city subject increasing debate year ur...</td>\n",
       "      <td>1</td>\n",
       "      <td>[7718, 5787, 1748, 2426, 3649, 4384, 614, 7876...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>car free city carfree city concept gaining tra...</td>\n",
       "      <td>1</td>\n",
       "      <td>[7718, 1479, 1748, 1097, 5787, 1748, 3721, 139...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sustainable urban future carfree city emerging...</td>\n",
       "      <td>1</td>\n",
       "      <td>[82, 24196, 7876, 2003, 1097, 5787, 1748, 1182...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pioneering sustainable urban living era marked...</td>\n",
       "      <td>1</td>\n",
       "      <td>[79, 7935, 1586, 13347, 7876, 2877, 6980, 7498...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>path sustainable urban living age rapid urbani...</td>\n",
       "      <td>1</td>\n",
       "      <td>[6978, 13347, 7876, 2877, 2479, 5801, 7876, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29140</th>\n",
       "      <td>ha fuss elector college people confused work s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[3099, 34297, 43191, 4152, 661, 10416, 670, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29141</th>\n",
       "      <td>limiting car usage ha advantage putting lot po...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2475, 1780, 1097, 8748, 387, 4621, 5137, 1256...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29142</th>\n",
       "      <td>trend ha developing year throttle affect mass ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[83, 10920, 387, 5922, 614, 29976, 2689, 2347,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29143</th>\n",
       "      <td>car big society today car bigger impact people...</td>\n",
       "      <td>0</td>\n",
       "      <td>[7718, 1263, 3592, 1909, 1097, 5749, 2928, 661...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29144</th>\n",
       "      <td>car 1800s popular year number car bought licen...</td>\n",
       "      <td>0</td>\n",
       "      <td>[7718, 21431, 82, 2968, 614, 1271, 1097, 5839,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29145 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  generated  \\\n",
       "0      carfree city subject increasing debate year ur...          1   \n",
       "1      car free city carfree city concept gaining tra...          1   \n",
       "2      sustainable urban future carfree city emerging...          1   \n",
       "3      pioneering sustainable urban living era marked...          1   \n",
       "4      path sustainable urban living age rapid urbani...          1   \n",
       "...                                                  ...        ...   \n",
       "29140  ha fuss elector college people confused work s...          0   \n",
       "29141  limiting car usage ha advantage putting lot po...          0   \n",
       "29142  trend ha developing year throttle affect mass ...          0   \n",
       "29143  car big society today car bigger impact people...          0   \n",
       "29144  car 1800s popular year number car bought licen...          0   \n",
       "\n",
       "                                            encoded_text  \n",
       "0      [7718, 5787, 1748, 2426, 3649, 4384, 614, 7876...  \n",
       "1      [7718, 1479, 1748, 1097, 5787, 1748, 3721, 139...  \n",
       "2      [82, 24196, 7876, 2003, 1097, 5787, 1748, 1182...  \n",
       "3      [79, 7935, 1586, 13347, 7876, 2877, 6980, 7498...  \n",
       "4      [6978, 13347, 7876, 2877, 2479, 5801, 7876, 16...  \n",
       "...                                                  ...  \n",
       "29140  [3099, 34297, 43191, 4152, 661, 10416, 670, 11...  \n",
       "29141  [2475, 1780, 1097, 8748, 387, 4621, 5137, 1256...  \n",
       "29142  [83, 10920, 387, 5922, 614, 29976, 2689, 2347,...  \n",
       "29143  [7718, 1263, 3592, 1909, 1097, 5749, 2928, 661...  \n",
       "29144  [7718, 21431, 82, 2968, 614, 1271, 1097, 5839,...  \n",
       "\n",
       "[29145 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"encoded_text\"] = df[\"text\"].apply(tokenizer.encode)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0ff7dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 170.07335735117516, Median: 160.0\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = [len(seq) for seq in df['encoded_text']]\n",
    "print(f\"Mean: {np.mean(seq_lengths)}, Median: {np.median(seq_lengths)}\")\n",
    "maxlen = int(np.percentile(seq_lengths, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37f6d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, maxlen=546):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = torch.tensor(self.texts[idx][:self.maxlen], dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8a861f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-27 13:54:26.851739: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753613666.865064   18087 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753613666.869012   18087 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753613666.880494   18087 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753613666.880508   18087 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753613666.880510   18087 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753613666.880511   18087 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-27 13:54:26.884487: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95a60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9faa540",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(df['encoded_text'], maxlen=maxlen, padding='post', truncating='post')\n",
    "y = df['generated'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d6bb1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c13ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset ve DataLoader oluÅŸtur\n",
    "train_dataset = TextDataset(X_train, y_train, maxlen=maxlen)\n",
    "test_dataset = TextDataset(X_test, y_test, maxlen=maxlen)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c02ef4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextClassifier(\n",
       "  (embedding): Embedding(50257, 512, padding_idx=0)\n",
       "  (lstm): LSTM(512, 64, num_layers=4, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.7, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=512, hidden_dim=64):\n",
    "        super (TextClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True, bidirectional=True, num_layers=4)\n",
    "        self.fc = nn.Linear(in_features=hidden_dim*2, out_features=1)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        output = self.fc(self.dropout(last_hidden))\n",
    "\n",
    "        return torch.sigmoid(output)\n",
    "    \n",
    "model = TextClassifier(vocab_size=tokenizer.vocab_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25b9b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device=device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63302e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 0.5784, Accuracy = 0.8670\n",
      "Epoch 2: Loss = 0.2743, Accuracy = 0.9499\n",
      "Epoch 3: Loss = 0.1714, Accuracy = 0.9693\n",
      "Epoch 4: Loss = 0.1184, Accuracy = 0.9708\n",
      "Epoch 5: Loss = 0.1048, Accuracy = 0.9798\n",
      "Epoch 6: Loss = 0.0741, Accuracy = 0.9825\n",
      "Epoch 7: Loss = 0.0490, Accuracy = 0.9825\n",
      "Epoch 8: Loss = 0.0415, Accuracy = 0.9842\n",
      "Epoch 9: Loss = 0.0324, Accuracy = 0.9878\n",
      "Epoch 10: Loss = 0.0241, Accuracy = 0.9876\n",
      "Epoch 11: Loss = 0.0218, Accuracy = 0.9880\n",
      "Epoch 12: Loss = 0.0178, Accuracy = 0.9899\n",
      "Epoch 13: Loss = 0.0193, Accuracy = 0.9907\n",
      "Epoch 14: Loss = 0.0150, Accuracy = 0.9897\n",
      "Epoch 15: Loss = 0.0097, Accuracy = 0.9900\n",
      "Epoch 16: Loss = 0.0103, Accuracy = 0.9870\n",
      "Epoch 17: Loss = 0.0081, Accuracy = 0.9902\n",
      "Epoch 18: Loss = 0.0066, Accuracy = 0.9892\n",
      "Epoch 19: Loss = 0.0078, Accuracy = 0.9916\n",
      "Epoch 20: Loss = 0.0076, Accuracy = 0.9899\n",
      "Epoch 21: Loss = 0.0121, Accuracy = 0.9894\n",
      "Epoch 22: Loss = 0.0102, Accuracy = 0.9878\n",
      "Epoch 23: Loss = 0.0056, Accuracy = 0.9921\n",
      "Epoch 24: Loss = 0.0042, Accuracy = 0.9926\n",
      "Epoch 25: Loss = 0.0022, Accuracy = 0.9925\n",
      "Epoch 26: Loss = 0.0014, Accuracy = 0.9928\n",
      "Epoch 27: Loss = 0.0009, Accuracy = 0.9925\n",
      "Epoch 28: Loss = 0.0008, Accuracy = 0.9926\n",
      "Epoch 29: Loss = 0.0010, Accuracy = 0.9928\n",
      "Epoch 30: Loss = 0.0008, Accuracy = 0.9928\n",
      "Epoch 31: Loss = 0.0014, Accuracy = 0.9906\n",
      "Epoch 32: Loss = 0.0007, Accuracy = 0.9926\n",
      "Epoch 33: Loss = 0.0070, Accuracy = 0.9904\n",
      "Epoch 34: Loss = 0.0037, Accuracy = 0.9918\n",
      "Epoch 35: Loss = 0.0044, Accuracy = 0.9928\n",
      "Epoch 36: Loss = 0.0035, Accuracy = 0.9926\n",
      "Epoch 37: Loss = 0.0022, Accuracy = 0.9928\n",
      "Epoch 38: Loss = 0.0027, Accuracy = 0.9701\n",
      "Epoch 39: Loss = 0.0036, Accuracy = 0.9921\n",
      "Epoch 40: Loss = 0.0016, Accuracy = 0.9930\n",
      "Epoch 41: Loss = 0.0007, Accuracy = 0.9926\n",
      "Epoch 42: Loss = 0.0003, Accuracy = 0.9928\n",
      "Epoch 43: Loss = 0.0001, Accuracy = 0.9931\n",
      "Epoch 44: Loss = 0.0000, Accuracy = 0.9928\n",
      "Epoch 45: Loss = 0.0000, Accuracy = 0.9930\n",
      "Epoch 46: Loss = 0.0000, Accuracy = 0.9931\n",
      "Epoch 47: Loss = 0.0000, Accuracy = 0.9931\n",
      "Epoch 48: Loss = 0.0000, Accuracy = 0.9931\n",
      "Epoch 49: Loss = 0.0000, Accuracy = 0.9931\n",
      "Epoch 50: Loss = 0.0000, Accuracy = 0.9930\n",
      "Epoch 51: Loss = 0.0000, Accuracy = 0.9931\n",
      "Epoch 52: Loss = 0.0000, Accuracy = 0.9930\n",
      "Epoch 53: Loss = 0.0000, Accuracy = 0.9930\n",
      "Epoch 54: Loss = 0.0000, Accuracy = 0.9930\n",
      "Epoch 55: Loss = 0.0000, Accuracy = 0.9930\n",
      "Epoch 56: Loss = 0.0000, Accuracy = 0.9926\n",
      "Epoch 57: Loss = 0.0000, Accuracy = 0.9928\n",
      "Epoch 58: Loss = 0.0000, Accuracy = 0.9928\n",
      "Epoch 59: Loss = 0.0000, Accuracy = 0.9928\n",
      "Epoch 60: Loss = 0.0000, Accuracy = 0.9926\n",
      "Epoch 61: Loss = 0.0220, Accuracy = 0.9925\n",
      "Epoch 62: Loss = 0.0049, Accuracy = 0.9926\n",
      "Epoch 63: Loss = 0.0017, Accuracy = 0.9931\n",
      "Epoch 64: Loss = 0.0076, Accuracy = 0.9851\n",
      "Epoch 65: Loss = 0.0070, Accuracy = 0.9914\n",
      "Epoch 66: Loss = 0.0029, Accuracy = 0.9902\n",
      "Epoch 67: Loss = 0.0017, Accuracy = 0.9933\n",
      "Epoch 68: Loss = 0.0008, Accuracy = 0.9928\n",
      "Epoch 69: Loss = 0.0014, Accuracy = 0.9938\n",
      "Epoch 70: Loss = 0.0008, Accuracy = 0.9928\n",
      "Epoch 71: Loss = 0.0013, Accuracy = 0.9930\n",
      "Epoch 72: Loss = 0.0043, Accuracy = 0.9923\n",
      "Epoch 73: Loss = 0.0020, Accuracy = 0.9904\n",
      "Epoch 74: Loss = 0.0010, Accuracy = 0.9899\n",
      "Epoch 75: Loss = 0.0002, Accuracy = 0.9919\n",
      "Epoch 76: Loss = 0.0001, Accuracy = 0.9911\n",
      "Epoch 77: Loss = 0.0002, Accuracy = 0.9925\n",
      "Epoch 78: Loss = 0.0000, Accuracy = 0.9926\n",
      "Epoch 79: Loss = 0.0000, Accuracy = 0.9926\n",
      "Epoch 80: Loss = 0.0000, Accuracy = 0.9925\n",
      "Epoch 81: Loss = 0.0000, Accuracy = 0.9921\n",
      "Epoch 82: Loss = 0.0000, Accuracy = 0.9919\n",
      "Epoch 83: Loss = 0.0000, Accuracy = 0.9919\n",
      "Epoch 84: Loss = 0.0000, Accuracy = 0.9916\n",
      "Epoch 85: Loss = 0.0000, Accuracy = 0.9916\n",
      "Epoch 86: Loss = 0.0000, Accuracy = 0.9918\n",
      "Epoch 87: Loss = 0.0000, Accuracy = 0.9918\n",
      "Epoch 88: Loss = 0.0000, Accuracy = 0.9919\n",
      "Epoch 89: Loss = 0.0000, Accuracy = 0.9916\n",
      "Epoch 90: Loss = 0.0044, Accuracy = 0.9928\n",
      "Epoch 91: Loss = 0.0073, Accuracy = 0.9926\n",
      "Epoch 92: Loss = 0.0020, Accuracy = 0.9926\n",
      "Epoch 93: Loss = 0.0006, Accuracy = 0.9931\n",
      "Epoch 94: Loss = 0.0003, Accuracy = 0.9930\n",
      "Epoch 95: Loss = 0.0002, Accuracy = 0.9928\n",
      "Epoch 96: Loss = 0.0000, Accuracy = 0.9931\n",
      "Epoch 97: Loss = 0.0000, Accuracy = 0.9933\n",
      "Epoch 98: Loss = 0.0000, Accuracy = 0.9931\n",
      "Epoch 99: Loss = 0.0000, Accuracy = 0.9931\n",
      "Epoch 100: Loss = 0.0000, Accuracy = 0.9931\n"
     ]
    }
   ],
   "source": [
    "def train_model(epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for text, label in train_loader:\n",
    "            text, label = text.to(device=device), label.to(device=device)\n",
    "            label = label.unsqueeze(1).float()\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(text)\n",
    "            loss = loss_fn(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()   \n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for texts, labels in test_loader:\n",
    "                texts, labels = texts.to(device), labels.to(device)\n",
    "                preds = (model(texts).squeeze() > 0.5).float()\n",
    "                correct += (preds == labels).sum().item()\n",
    "        \n",
    "        acc = correct / len(test_dataset)\n",
    "        print(f'Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}, Accuracy = {acc:.4f}')\n",
    "\n",
    "train_model(epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "46562ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      3539\n",
      "           1       0.99      0.99      0.99      2290\n",
      "\n",
      "    accuracy                           0.99      5829\n",
      "   macro avg       0.99      0.99      0.99      5829\n",
      "weighted avg       0.99      0.99      0.99      5829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in test_loader:\n",
    "            texts = texts.to(device)\n",
    "            preds = (model(texts).squeeze() > 0.5).int().cpu()\n",
    "            y_true.extend(labels.int().tolist())\n",
    "            y_pred.extend(preds.tolist())\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e77eebb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The old lighthouse stood sentinel on the craggy cliffs, its beam sweeping across the turbulent sea. Generations of keepers had climbed its winding stairs, their lives dedicated to guiding ships safely through treacherous waters. Though automated now, a sense of enduring vigilance still clung to its weathered stone walls, a silent testament to countless storms weathered and lives saved.\n",
      "Guess: AI (%100.00 confidence)\n"
     ]
    }
   ],
   "source": [
    "def predict(text, model, tokenizer, device='cpu'):\n",
    "    # 1. Metni tokenize et (input_ids tensor'Ä± elde edilir)\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "\n",
    "    # 2. Modeli deÄŸerlendirme moduna al ve tahmin yap\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)       # Ã‡Ä±ktÄ±: sigmoid uygulanmÄ±ÅŸ [0,1] arasÄ± tek deÄŸer\n",
    "        prob = output.item()            # Bu deÄŸer, metnin \"AI tarafÄ±ndan yazÄ±lmÄ±ÅŸ olma olasÄ±lÄ±ÄŸÄ±dÄ±r\"\n",
    "\n",
    "    # 3. SÄ±nÄ±f belirleme\n",
    "    label = \"AI\" if prob >= 0.5 else \"Human\"\n",
    "\n",
    "    # 4. Tahmini, ait olduÄŸu sÄ±nÄ±fa gÃ¶re yorumla:\n",
    "    confidence = prob if label == \"AI\" else (1 - prob)\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"label\": label,\n",
    "        \"confidence\": confidence  # Modelin bu sÄ±nÄ±fa olan gÃ¼veni (0-1)\n",
    "    }\n",
    "\n",
    "# Ã–rnek kullanÄ±m\n",
    "sentence = \"The old lighthouse stood sentinel on the craggy cliffs, its beam sweeping across the turbulent sea. Generations of keepers had climbed its winding stairs, their lives dedicated to guiding ships safely through treacherous waters. Though automated now, a sense of enduring vigilance still clung to its weathered stone walls, a silent testament to countless storms weathered and lives saved.\"\n",
    "result = predict(sentence, model, tokenizer, device=device)\n",
    "\n",
    "print(f\"Text: {result['text']}\")\n",
    "print(f\"Guess: {result['label']} (%{result['confidence']*100:.2f} confidence)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
